# -*- coding: utf-8 -*-
"""AllQueries.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1JZ8_SocKe3bzEaUdW0uWB8eD3CwnxQDC
"""

from pyspark.sql import SparkSession
from pyspark.sql.functions import from_json, col, sum, count, explode, year, expr, udf
from pyspark.sql.types import ArrayType,MapType, StringType, IntegerType, DoubleType,DateType,FloatType, StructType, StructField,DecimalType
import itertools
import pandas as pd

#@title üîì Crear session de spark
spark = SparkSession.builder.getOrCreate()

#@title ü•Ω Cargar datos
path_movies="hdfs://ip-172-31-35-103.ec2.internal:9000/proyecto/data/tmdb_5000_movies.csv"
path_credits="hdfs://ip-172-31-35-103.ec2.internal:9000/proyecto/data/tmdb_5000_credits.csv"
df_movies = spark.read.csv(path_movies,header=True, inferSchema=True, quote='"', escape='"', multiLine=True)
df_credits = spark.read.csv(path_credits,header=True, inferSchema=True, quote='"', escape='"', multiLine=True)





#@title üîùüé¨üíµ  1. Obtener las 10 productoras que m√°s beneficios han generado (2%)
# Definir el esquema para la columna 'production_companies'
company_schema = ArrayType(StructType([
    StructField("name", StringType(), True),
    StructField("id", IntegerType(), True)
]))
df_movies = df_movies.withColumn("benefits", expr("revenue - budget"))
# Explotar la columna 'production_companies' para obtener cada productora por separado
df_expanded = df_movies.select("benefits", explode(from_json(col("production_companies"), company_schema)).alias("company"))
# Agrupar por productora y calcular los beneficios totales
df_grouped = df_expanded.groupBy("company.id").agg(sum("benefits").alias("total_benefits"))
# Sacamos un df con las compa√≠as y su id sin repeticiones
df_companies = df_expanded.select("company.name", "company.id").dropDuplicates(["name", "id"])
# Juntamos el groupby con el nombre de la compa√±√≠a
joined_df = df_companies.join(df_grouped, df_companies.id == df_grouped.id).drop(df_companies['id'])
# Ordenar de forma descendente y obtener las 10 productoras principales
top_producers = joined_df.orderBy(col("total_benefits").desc()).limit(10)
df.write.parquet("hdfs://ip-172-31-35-103.ec2.internal:9000/proyecto/consulta1_parquet")


